{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pvlib\n",
    "from pvlib import location\n",
    "\n",
    "# Define date range for GMT+7 (Asia/Saigon)\n",
    "start_date = pd.Timestamp(\"2025-01-01 00:00:00\", tz=\"Asia/Saigon\")\n",
    "end_date = pd.Timestamp(\"2025-03-14 23:59:59\", tz=\"Asia/Saigon\")\n",
    "\n",
    "# Using existing site location\n",
    "lat, lon = 10.76477848, 106.3148294\n",
    "site = location.Location(lat, lon, tz=\"Asia/Saigon\")\n",
    "\n",
    "# Generate time range with 10-minute intervals\n",
    "times = pd.date_range(start=start_date, end=end_date, freq=\"10min\", tz=\"Asia/Saigon\")\n",
    "\n",
    "# Calculate clear sky data\n",
    "clearsky_data = site.get_clearsky(times)\n",
    "clearsky_data.reset_index(inplace=True)\n",
    "clearsky_data.rename(columns={\"index\": \"Time\"}, inplace=True)\n",
    "clearsky_data.Time = pd.to_datetime(clearsky_data.Time).dt.tz_localize(None)\n",
    "\n",
    "# Index l√† c·ªôt Time, t·∫°o m·ªôt 'GHI_CS' m·ªõi t·ª´ c·ªôt 'ghi' c·ªßa clearsky_data v√† xo√° c√°c c·ªôt c√≤n l·∫°i\n",
    "clearsky_data.set_index(\"Time\", inplace=True)\n",
    "clearsky_data = clearsky_data[[\"ghi\"]]\n",
    "clearsky_data.rename(columns={\"ghi\": \"GHI_CS\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clearsky_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 1: Import th∆∞ vi·ªán v√† c√†i ƒë·∫∑t\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "sys.path.append(\"P:/7. User/Tuong\")\n",
    "\n",
    "from DB_ALL import SQL_NLTT_NOIBO, Database_TTD\n",
    "\n",
    "PATH = r\"Z:\\Sky-image\\namnvn\\Data_process\\MT Solarpark 1\"\n",
    "POWER_CSV = r\"C:\\Khue\\H9_Solar_Power_Forecasting\\data\\processed\\df_353.csv\"\n",
    "SEQ_LENGTH = 24\n",
    "PRED_LENGTH = 24\n",
    "IMG_SIZE = (32, 32)\n",
    "NUM_CHANNELS = 4\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "\n",
    "START_DATE = datetime.datetime(2025, 1, 1)\n",
    "END_DATE = datetime.datetime(2025, 1, 14)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p c·∫•u h√¨nh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 2: X·ª≠ l√Ω d·ªØ li·ªáu c√¥ng su·∫•t\n",
    "print(\"üîÑ ƒêang load d·ªØ li·ªáu c√¥ng su·∫•t...\")\n",
    "power_df = pd.read_csv(POWER_CSV)\n",
    "power_df.reset_index(inplace=True)\n",
    "power_df[\"Time\"] = pd.to_datetime(power_df[\"Time\"])\n",
    "scaler = MinMaxScaler()\n",
    "power_df[\"P_uoc_normalized\"] = scaler.fit_transform(power_df[[\"P_uoc\"]])\n",
    "power_lookup = power_df.set_index(\"Time\")[\"P_uoc_normalized\"].to_dict()\n",
    "print(f\"‚úÖ ƒê√£ load {len(power_df)} b·∫£n ghi\")\n",
    "\n",
    "print(\"D·ªØ li·ªáu m·∫´u:\", power_df.head())\n",
    "print(\"Lookup m·∫´u:\", {k: power_lookup[k] for k in list(power_lookup.keys())[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 3.1: X·ª≠ l√Ω ·∫£nh th√†nh chu·ªói d·ªØ li·ªáu multi-channel\n",
    "def load_and_process_image(path):\n",
    "    try:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_image(img, channels=1, expand_animations=False)\n",
    "        img = tf.image.resize(img, IMG_SIZE)\n",
    "        img = (img / 127.5) - 1.0\n",
    "        return img.numpy()\n",
    "    except:\n",
    "        return np.zeros(IMG_SIZE + (1,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def preprocess_images(image_records_dict):\n",
    "    print(\"\\nüîÑ ƒêang x·ª≠ l√Ω ·∫£nh th√†nh chu·ªói d·ªØ li·ªáu multi-channel...\")\n",
    "    prefixes = [\"b03_\", \"b07_\", \"b08_\", \"b13_\"]\n",
    "    sequences_images = []\n",
    "    sequences_times = []\n",
    "\n",
    "    # T√¨m th·ªùi gian chung\n",
    "    times = set.intersection(\n",
    "        *(set(dt for _, dt in image_records_dict[prefix]) for prefix in prefixes)\n",
    "    )\n",
    "    times = sorted(times)\n",
    "    min_length = len(times) - SEQ_LENGTH - PRED_LENGTH + 1\n",
    "\n",
    "    for i in range(max(0, min_length)):\n",
    "        time_window = times[i : i + SEQ_LENGTH]\n",
    "        if len(time_window) != SEQ_LENGTH:  # B·ªè qua n·∫øu kh√¥ng ƒë·ªß SEQ_LENGTH\n",
    "            continue\n",
    "        multi_channel_seq = []\n",
    "        for prefix in prefixes:\n",
    "            channel_seq = []\n",
    "            for t in time_window:\n",
    "                path = next(\n",
    "                    (p for p, dt in image_records_dict[prefix] if dt == t), None\n",
    "                )\n",
    "                img = (\n",
    "                    load_and_process_image(path)\n",
    "                    if path\n",
    "                    else np.zeros(IMG_SIZE + (1,), dtype=np.float32)\n",
    "                )\n",
    "                channel_seq.append(img.squeeze())  # Lo·∫°i b·ªè chi·ªÅu (1)\n",
    "            multi_channel_seq.append(channel_seq)\n",
    "        # Stack th√†nh (SEQ_LENGTH, 32, 32, 4)\n",
    "        stacked_seq = np.stack(multi_channel_seq, axis=-1)\n",
    "        sequences_images.append(stacked_seq)\n",
    "        sequences_times.append(time_window[-1])\n",
    "\n",
    "    sequences_images = np.array(sequences_images)\n",
    "    print(f\"‚úÖ ƒê√£ x·ª≠ l√Ω {len(sequences_images)} chu·ªói ·∫£nh\")\n",
    "    print(\"Shape c·ªßa sequences_images:\", sequences_images.shape)\n",
    "    if sequences_images.size > 0:\n",
    "        print(\n",
    "            \"M·∫´u ƒë·∫ßu ti√™n (min, max):\",\n",
    "            sequences_images[0].min(),\n",
    "            sequences_images[0].max(),\n",
    "        )\n",
    "    return sequences_images, sequences_times\n",
    "\n",
    "\n",
    "# T·∫°o image_records\n",
    "date_dirs = [d.strftime(\"%Y%m%d\") for d in pd.date_range(START_DATE, END_DATE)]\n",
    "image_records_dict = {\"b03_\": [], \"b07_\": [], \"b08_\": [], \"b13_\": []}\n",
    "for date_dir in date_dirs:\n",
    "    date_path = os.path.join(PATH, date_dir)\n",
    "    if os.path.exists(date_path):\n",
    "        for time_dir in os.listdir(date_path):\n",
    "            time_path = os.path.join(date_path, time_dir)\n",
    "            if os.path.isdir(time_path):\n",
    "                try:\n",
    "                    dt = datetime.datetime.strptime(\n",
    "                        f\"{date_dir}T{time_dir}\", \"%Y%m%dT%H%M\"\n",
    "                    )\n",
    "                    for file in os.listdir(time_path):\n",
    "                        for prefix in image_records_dict.keys():\n",
    "                            if file.lower().startswith(\n",
    "                                prefix\n",
    "                            ) and file.lower().endswith(\n",
    "                                (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n",
    "                            ):\n",
    "                                img_path = os.path.join(time_path, file)\n",
    "                                image_records_dict[prefix].append((img_path, dt))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "for prefix in image_records_dict:\n",
    "    image_records_dict[prefix].sort(key=lambda x: x[1])\n",
    "\n",
    "sequences_images_processed, sequences_times = preprocess_images(image_records_dict)\n",
    "\n",
    "# Ki·ªÉm tra\n",
    "if sequences_images_processed.size > 0:\n",
    "    for i, prefix in enumerate([\"b03_\", \"b07_\", \"b08_\", \"b13_\"]):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.imshow(sequences_images_processed[0][0, :, :, i], cmap=\"gray\")\n",
    "        plt.title(f\"K√™nh {prefix}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Th·ªùi gian m·∫´u:\", [t.strftime(\"%Y-%m-%d %H:%M\") for t in sequences_times[:5]])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ chu·ªói ·∫£nh n√†o ƒë∆∞·ª£c t·∫°o!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 3.2: Gh√©p v·ªõi P_uoc v√† t·∫°o dataset\n",
    "def prepare_dataset_with_processed_images(sequences_images_processed, sequences_times):\n",
    "    print(\"\\nüîÑ ƒêang gh√©p d·ªØ li·ªáu ·∫£nh v·ªõi P_uoc...\")\n",
    "    sequences_p_uoc = []\n",
    "    targets = []\n",
    "    valid_sequences_images = []\n",
    "    valid_sequences_times = []\n",
    "    for i, (img_seq, seq_time) in enumerate(\n",
    "        zip(sequences_images_processed, sequences_times)\n",
    "    ):\n",
    "        start_idx = i\n",
    "        p_uoc_seq = [\n",
    "            power_lookup.get(image_records_dict[\"b03_\"][start_idx + j][1], 0.0)\n",
    "            for j in range(SEQ_LENGTH)\n",
    "        ]\n",
    "        target_dts = [\n",
    "            image_records_dict[\"b03_\"][start_idx + SEQ_LENGTH + j][1]\n",
    "            for j in range(PRED_LENGTH)\n",
    "        ]\n",
    "        target_powers = [power_lookup.get(dt, -1.0) for dt in target_dts]\n",
    "        if all(power != -1.0 for power in target_powers):\n",
    "            valid_sequences_images.append(img_seq)\n",
    "            sequences_p_uoc.append(p_uoc_seq)\n",
    "            targets.append(target_powers)\n",
    "            valid_sequences_times.append(seq_time)\n",
    "\n",
    "    print(\"\\nV√≠ d·ª• ƒë·∫ßu v√†o v√† ƒë·∫ßu ra:\")\n",
    "    print(\"P_uoc ƒë·∫ßu v√†o:\", sequences_p_uoc[0])\n",
    "    print(\"P_uoc ƒë·∫ßu ra:\", targets[0])\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            (np.array(valid_sequences_images), np.array(sequences_p_uoc)),\n",
    "            np.array(targets),\n",
    "        )\n",
    "    )\n",
    "    ds = ds.map(lambda x, y: ((x[0], x[1]), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    final_ds = ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    print(f\"‚úÖ Dataset ƒë√£ s·∫µn s√†ng v·ªõi {len(valid_sequences_images)} m·∫´u\")\n",
    "    return final_ds, valid_sequences_times\n",
    "\n",
    "\n",
    "dataset, sequences_times_train = prepare_dataset_with_processed_images(\n",
    "    sequences_images_processed, sequences_times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 4: ƒê·ªãnh nghƒ©a m√¥ h√¨nh\n",
    "def build_model():\n",
    "    input_images = layers.Input(\n",
    "        shape=(SEQ_LENGTH, IMG_SIZE[0], IMG_SIZE[1], NUM_CHANNELS)\n",
    "    )\n",
    "    x = layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation=\"relu\"))(\n",
    "        input_images\n",
    "    )\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D(2, 2))(x)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation=\"relu\"))(x)\n",
    "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\n",
    "    input_p_uoc = layers.Input(shape=(SEQ_LENGTH,))\n",
    "    p_uoc_features = layers.Reshape((SEQ_LENGTH, 1))(input_p_uoc)\n",
    "    combined = layers.Concatenate(axis=-1)([x, p_uoc_features])\n",
    "    lstm_out = layers.LSTM(128, return_sequences=False)(combined)\n",
    "    output = layers.Dense(PRED_LENGTH)(lstm_out)\n",
    "    model = models.Model(inputs=[input_images, input_p_uoc], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001), loss=\"mae\", metrics=[\"mae\"]\n",
    "    )\n",
    "    print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c build\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 5: Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "def train_model(dataset):\n",
    "    model = build_model()\n",
    "    print(\"\\nüîÑ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...\")\n",
    "    history = model.fit(dataset, epochs=EPOCHS, verbose=1)\n",
    "    print(\"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t\")\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model, history = train_model(dataset)\n",
    "plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
    "plt.plot(history.history[\"mae\"], label=\"MAE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Training Loss and MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def test_forecast_interval(model, scaler, test_date, input_start_hour, input_end_hour):\n",
    "    \"\"\"\n",
    "    Test d·ª± b√°o c√¥ng su·∫•t cho m·ªôt kho·∫£ng th·ªùi gian c·ª• th·ªÉ tr√™n m·ªôt ng√†y.\n",
    "\n",
    "    ƒê·∫ßu v√†o:\n",
    "      - model: m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán.\n",
    "      - scaler: ƒë·ªëi t∆∞·ª£ng MinMaxScaler ƒë√£ d√πng ƒë·ªÉ chu·∫©n h√≥a c√¥ng su·∫•t.\n",
    "      - test_date: ng√†y test (v√≠ d·ª•: datetime.datetime(2025, 1, 18)).\n",
    "      - input_start_hour: gi·ªù b·∫Øt ƒë·∫ßu ƒë·∫ßu v√†o (v√≠ d·ª•: 10).\n",
    "      - input_end_hour: gi·ªù k·∫øt th√∫c ƒë·∫ßu v√†o (v√≠ d·ª•: 14).\n",
    "\n",
    "    Gi·∫£ s·ª≠ th·ªùi gian ƒë·∫ßu v√†o v√† th·ªùi gian forecast c√≥ ƒë·ªô d√†i b·∫±ng nhau.\n",
    "    V·ªõi m·ªói b∆∞·ªõc c√°ch nhau 10 ph√∫t, n√™n:\n",
    "      SEQ_LENGTH = s·ªë b∆∞·ªõc ƒë·∫ßu v√†o = (input_end_hour - input_start_hour)*6\n",
    "      PRED_LENGTH = s·ªë b∆∞·ªõc forecast = (input_end_hour - input_start_hour)*6\n",
    "\n",
    "    Tr·∫£ v·ªÅ:\n",
    "      - DataFrame c√≥ ch·ªâ s·ªë th·ªùi gian, ch·ª©a gi√° tr·ªã d·ª± b√°o v√† gi√° tr·ªã th·ª±c (n·∫øu c√≥) cho kho·∫£ng forecast.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # X√°c ƒë·ªãnh kho·∫£ng th·ªùi gian ƒë·∫ßu v√†o v√† forecast\n",
    "    input_duration = input_end_hour - input_start_hour  # v√≠ d·ª•: 14 - 10 = 4 gi·ªù\n",
    "    forecast_end_hour = input_end_hour + input_duration  # v√≠ d·ª•: 14 + 4 = 18 gi·ªù\n",
    "\n",
    "    # X√¢y d·ª±ng c√°c m·ªëc th·ªùi gian\n",
    "    test_date_only = test_date.date()\n",
    "    input_start_dt = datetime.datetime.combine(\n",
    "        test_date_only, datetime.time(input_start_hour, 0)\n",
    "    )\n",
    "    forecast_end_dt = datetime.datetime.combine(\n",
    "        test_date_only, datetime.time(forecast_end_hour, 0)\n",
    "    )\n",
    "    total_steps = int(\n",
    "        ((forecast_end_dt - input_start_dt).seconds) / 600\n",
    "    )  # m·ªói b∆∞·ªõc 10 ph√∫t\n",
    "\n",
    "    # Ki·ªÉm tra t·ªïng s·ªë b∆∞·ªõc c√≥ b·∫±ng t·ªïng SEQ_LENGTH + PRED_LENGTH kh√¥ng\n",
    "    if total_steps != (SEQ_LENGTH + PRED_LENGTH):\n",
    "        print(\n",
    "            f\"Warning: T·ªïng s·ªë b∆∞·ªõc ({total_steps}) kh√°c v·ªõi SEQ_LENGTH+PRED_LENGTH ({SEQ_LENGTH + PRED_LENGTH}).\"\n",
    "        )\n",
    "\n",
    "    # Danh s√°ch c√°c timestamp cho to√†n b·ªô kho·∫£ng (t·ª´ 10h ƒë·∫øn 18h)\n",
    "    full_timestamps = [\n",
    "        input_start_dt + datetime.timedelta(minutes=10 * i) for i in range(total_steps)\n",
    "    ]\n",
    "\n",
    "    # --- Load d·ªØ li·ªáu ·∫£nh c·ªßa ng√†y test trong kho·∫£ng t·ª´ input_start_dt ƒë·∫øn forecast_end_dt ---\n",
    "    test_image_records_dict = {\"b03_\": [], \"b07_\": [], \"b08_\": [], \"b13_\": []}\n",
    "    date_dir = test_date.strftime(\"%Y%m%d\")\n",
    "    date_path = os.path.join(PATH, date_dir)\n",
    "    if not os.path.exists(date_path):\n",
    "        print(f\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c d·ªØ li·ªáu cho ng√†y {date_dir}.\")\n",
    "        return None\n",
    "\n",
    "    for time_dir in os.listdir(date_path):\n",
    "        time_path = os.path.join(date_path, time_dir)\n",
    "        if os.path.isdir(time_path):\n",
    "            try:\n",
    "                dt = datetime.datetime.strptime(f\"{date_dir}T{time_dir}\", \"%Y%m%dT%H%M\")\n",
    "                # Ch·ªâ ch·ªçn c√°c timestamp trong kho·∫£ng t·ª´ input_start_dt ƒë·∫øn forecast_end_dt\n",
    "                if input_start_dt <= dt < forecast_end_dt:\n",
    "                    for file in os.listdir(time_path):\n",
    "                        for prefix in test_image_records_dict.keys():\n",
    "                            if file.lower().startswith(\n",
    "                                prefix\n",
    "                            ) and file.lower().endswith(\n",
    "                                (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n",
    "                            ):\n",
    "                                img_path = os.path.join(time_path, file)\n",
    "                                test_image_records_dict[prefix].append((img_path, dt))\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói x·ª≠ l√Ω th∆∞ m·ª•c {time_path}: {e}\")\n",
    "                continue\n",
    "    # S·∫Øp x·∫øp theo th·ªùi gian cho m·ªói k√™nh\n",
    "    for prefix in test_image_records_dict:\n",
    "        test_image_records_dict[prefix].sort(key=lambda x: x[1])\n",
    "\n",
    "    # --- X√¢y d·ª±ng chu·ªói ·∫£nh theo to√†n b·ªô kho·∫£ng (10h ƒë·∫øn 18h) ---\n",
    "    # V√¨ h√†m preprocess_images ban ƒë·∫ßu ch·ªâ t·∫°o ra c√°c chu·ªói c√≥ ƒë·ªô d√†i SEQ_LENGTH,\n",
    "    # ta t·ª± t·∫°o chu·ªói ƒë·∫ßy ƒë·ªß d·ª±a v√†o full_timestamps.\n",
    "    prefixes = [\"b03_\", \"b07_\", \"b08_\", \"b13_\"]\n",
    "    # T·∫°o dictionary cho t·ª´ng k√™nh: mapping timestamp -> image (n·∫øu c√≥)\n",
    "    image_dict = {prefix: {} for prefix in prefixes}\n",
    "    for prefix in prefixes:\n",
    "        for path, ts in test_image_records_dict[prefix]:\n",
    "            image_dict[prefix][ts] = load_and_process_image(path)\n",
    "\n",
    "    full_seq = []\n",
    "    for ts in full_timestamps:\n",
    "        multi_channel_imgs = []\n",
    "        for prefix in prefixes:\n",
    "            # N·∫øu kh√¥ng c√≥ ·∫£nh t·∫°i th·ªùi ƒëi·ªÉm ts, thay b·∫±ng m·∫£ng zeros\n",
    "            img = image_dict[prefix].get(\n",
    "                ts, np.zeros(IMG_SIZE + (1,), dtype=np.float32)\n",
    "            )\n",
    "            multi_channel_imgs.append(img.squeeze())\n",
    "        # Gh√©p c√°c k√™nh l·∫°i th√†nh m·∫£ng v·ªõi shape (IMG_SIZE[0], IMG_SIZE[1], NUM_CHANNELS)\n",
    "        img_stack = np.stack(multi_channel_imgs, axis=-1)\n",
    "        full_seq.append(img_stack)\n",
    "    full_seq = np.array(full_seq)  # shape: (total_steps, 32, 32, 4)\n",
    "\n",
    "    # T√°ch d·ªØ li·ªáu th√†nh ph·∫ßn ƒë·∫ßu v√†o v√† ph·∫ßn c·∫ßn d·ª± b√°o\n",
    "    input_images = full_seq[:SEQ_LENGTH]  # t·ª´ 10h ƒë·∫øn 14h\n",
    "    forecast_images = full_seq[\n",
    "        SEQ_LENGTH : SEQ_LENGTH + PRED_LENGTH\n",
    "    ]  # t·ª´ 14h ƒë·∫øn 18h (d√πng ƒë·ªÉ l·∫•y timestamp & gi√° tr·ªã th·ª±c n·∫øu c√≥)\n",
    "\n",
    "    # L·∫•y chu·ªói c√¥ng su·∫•t t∆∞∆°ng ·ª©ng t·ª´ power_lookup\n",
    "    input_power_seq = [power_lookup.get(ts, 0.0) for ts in full_timestamps[:SEQ_LENGTH]]\n",
    "    # Get true power values for forecast period and inverse transform them\n",
    "    true_forecast_power = [\n",
    "        power_lookup.get(ts, np.nan)\n",
    "        for ts in full_timestamps[SEQ_LENGTH : SEQ_LENGTH + PRED_LENGTH]\n",
    "    ]\n",
    "    true_forecast_power = np.array(true_forecast_power).reshape(-1, 1)\n",
    "    true_forecast_power = scaler.inverse_transform(true_forecast_power).flatten()\n",
    "\n",
    "    # Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh (th√™m chi·ªÅu batch)\n",
    "    x_image = np.expand_dims(input_images, axis=0)  # (1, SEQ_LENGTH, 32, 32, 4)\n",
    "    x_power = np.expand_dims(np.array(input_power_seq), axis=0)  # (1, SEQ_LENGTH)\n",
    "\n",
    "    # D·ª± b√°o v·ªõi m√¥ h√¨nh\n",
    "    y_pred = model.predict([x_image, x_power])\n",
    "    # y_pred c√≥ shape (1, PRED_LENGTH)\n",
    "    y_pred = y_pred[0]  # shape: (PRED_LENGTH,)\n",
    "    # ƒê∆∞a v·ªÅ thang ƒëo ban ƒë·∫ßu\n",
    "    y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # T·∫°o DataFrame v·ªõi ch·ªâ s·ªë th·ªùi gian cho ph·∫ßn forecast (14h - 18h)\n",
    "    forecast_timestamps = full_timestamps[SEQ_LENGTH : SEQ_LENGTH + PRED_LENGTH]\n",
    "    df_forecast = pd.DataFrame(\n",
    "        {\n",
    "            \"Timestamp\": forecast_timestamps,\n",
    "            \"Predicted_Power\": y_pred_original,\n",
    "            \"True_Power\": true_forecast_power,\n",
    "        }\n",
    "    )\n",
    "    df_forecast.set_index(\"Timestamp\", inplace=True)\n",
    "\n",
    "    return df_forecast\n",
    "\n",
    "\n",
    "# --- Test Case ---\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test cho ng√†y 18/01/2025, ƒë·∫ßu v√†o t·ª´ 10h ƒë·∫øn 14h, d·ª± b√°o t·ª´ 14h ƒë·∫øn 18h\n",
    "test_date = datetime.datetime(2025, 1, 18)\n",
    "input_start_hour = 10\n",
    "input_end_hour = 14\n",
    "\n",
    "df_result = test_forecast_interval(\n",
    "    model, scaler, test_date, input_start_hour, input_end_hour\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def test_multiple_intervals(model, scaler, test_date, num_intervals=3):\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán d·ª± b√°o nhi·ªÅu kho·∫£ng th·ªùi gian li√™n ti·∫øp, m·ªói kho·∫£ng 4 gi·ªù\n",
    "\n",
    "    Args:\n",
    "        model: M√¥ h√¨nh ƒë√£ train\n",
    "        scaler: MinMaxScaler object\n",
    "        test_date: Ng√†y test\n",
    "        num_intervals: S·ªë kho·∫£ng d·ª± b√°o (m·∫∑c ƒë·ªãnh l√† 3: 10-14h ‚Üí 14-18h ‚Üí 18-22h ‚Üí 22-02h)\n",
    "\n",
    "    Returns:\n",
    "        List c√°c DataFrame k·∫øt qu·∫£ d·ª± b√°o\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(num_intervals):\n",
    "        # T√≠nh gi·ªù b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c cho m·ªói kho·∫£ng\n",
    "        input_start = 10 + (i * 4)  # 10, 14, 18\n",
    "        input_end = input_start + 4  # 14, 18, 22\n",
    "\n",
    "        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p chuy·ªÉn ng√†y (sau 24h)\n",
    "        current_date = test_date\n",
    "        if input_start >= 24:\n",
    "            input_start -= 24\n",
    "            input_end -= 24\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "\n",
    "        print(f\"\\nD·ª± b√°o kho·∫£ng {i + 1}:\")\n",
    "        print(f\"ƒê·∫ßu v√†o: {input_start}h - {input_end}h\")\n",
    "        print(f\"D·ª± b√°o: {input_end}h - {input_end + 4}h\")\n",
    "\n",
    "        df_result = test_forecast_interval(\n",
    "            model, scaler, current_date, input_start, input_end\n",
    "        )\n",
    "\n",
    "        if df_result is not None:\n",
    "            results.append(df_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test v·ªõi 3 kho·∫£ng d·ª± b√°o li√™n ti·∫øp\n",
    "test_date = datetime.datetime(2025, 1, 18)\n",
    "forecast_results = test_multiple_intervals(model, scaler, test_date, num_intervals=3)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì k·∫øt qu·∫£\n",
    "if forecast_results:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, df in enumerate(forecast_results):\n",
    "        plt.plot(\n",
    "            df.index,\n",
    "            df[\"Predicted_Power\"],\n",
    "            label=f\"Predicted (Interval {i + 1})\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        plt.plot(df.index, df[\"True_Power\"], label=f\"Actual (Interval {i + 1})\")\n",
    "\n",
    "    plt.title(\"Forecast Results for Multiple Intervals\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "if df_result is not None:\n",
    "    print(\"K·∫øt qu·∫£ d·ª± b√°o:\")\n",
    "    print(df_result)\n",
    "\n",
    "    # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh d·ª± b√°o v√† gi√° tr·ªã th·ª±c (n·∫øu c√≥)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(\n",
    "        df_result.index,\n",
    "        df_result[\"Predicted_Power\"],\n",
    "        label=\"Predicted Power\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    if not df_result[\"True_Power\"].isnull().all():\n",
    "        plt.plot(\n",
    "            df_result.index, df_result[\"True_Power\"], label=\"True Power\", marker=\"x\"\n",
    "        )\n",
    "    plt.xlabel(\"Th·ªùi gian\")\n",
    "    plt.ylabel(\"C√¥ng su·∫•t\")\n",
    "    plt.title(\"D·ª± b√°o c√¥ng su·∫•t t·ª´ 14h ƒë·∫øn 18h ng√†y 18/01/2025\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ k·∫øt qu·∫£ d·ª± b√°o h·ª£p l·ªá.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t√≠nh mape\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Cell 6: Test model\n",
    "def test_model(model, scaler, test_start_date, test_end_date):\n",
    "    global START_DATE, END_DATE\n",
    "    START_DATE, END_DATE = test_start_date, test_end_date\n",
    "\n",
    "    date_dirs = [d.strftime(\"%Y%m%d\") for d in pd.date_range(START_DATE, END_DATE)]\n",
    "    test_image_records_dict = {\"b03_\": [], \"b07_\": [], \"b13_\": [], \"b08_\": []}\n",
    "    for date_dir in date_dirs:\n",
    "        date_path = os.path.join(PATH, date_dir)\n",
    "        if os.path.exists(date_path):\n",
    "            for time_dir in os.listdir(date_path):\n",
    "                time_path = os.path.join(date_path, time_dir)\n",
    "                if os.path.isdir(time_path):\n",
    "                    try:\n",
    "                        dt = datetime.datetime.strptime(\n",
    "                            f\"{date_dir}T{time_dir}\", \"%Y%m%dT%H%M\"\n",
    "                        )\n",
    "                        for file in os.listdir(time_path):\n",
    "                            for prefix in test_image_records_dict.keys():\n",
    "                                if file.lower().startswith(\n",
    "                                    prefix\n",
    "                                ) and file.lower().endswith(\n",
    "                                    (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n",
    "                                ):\n",
    "                                    img_path = os.path.join(time_path, file)\n",
    "                                    test_image_records_dict[prefix].append(\n",
    "                                        (img_path, dt)\n",
    "                                    )\n",
    "                    except:\n",
    "                        continue\n",
    "    for prefix in test_image_records_dict:\n",
    "        test_image_records_dict[prefix].sort(key=lambda x: x[1])\n",
    "\n",
    "    sequences_images_test, sequences_times_test = preprocess_images(\n",
    "        test_image_records_dict\n",
    "    )\n",
    "    test_dataset, sequences_times = prepare_dataset_with_processed_images(\n",
    "        sequences_images_test, sequences_times_test\n",
    "    )\n",
    "\n",
    "    filtered_indices = [i for i, t in enumerate(sequences_times) if 5 <= t.hour <= 19]\n",
    "    if not filtered_indices:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng 5h-19h!\")\n",
    "        return None, None\n",
    "\n",
    "    test_loss, test_mae = model.evaluate(test_dataset, verbose=1)\n",
    "    print(f\"‚úÖ Test Loss: {test_loss}\")\n",
    "    print(f\"‚úÖ Test MAE: {test_mae}\")\n",
    "\n",
    "    predictions = model.predict(test_dataset)\n",
    "    true_values = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)\n",
    "    true_values = scaler.inverse_transform(true_values)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "    filtered_true_values = true_values[filtered_indices]\n",
    "    filtered_predictions = predictions[filtered_indices]\n",
    "    filtered_times = [sequences_times[i] for i in filtered_indices]\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(\n",
    "        filtered_times,\n",
    "        filtered_true_values[:, 0],\n",
    "        label=\"Gi√° tr·ªã th·ª±c t·∫ø b∆∞·ªõc 1\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtered_times,\n",
    "        filtered_predictions[:, 0],\n",
    "        label=\"Gi√° tr·ªã d·ª± ƒëo√°n b∆∞·ªõc 1\",\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtered_times,\n",
    "        filtered_true_values[:, 5],\n",
    "        label=\"Gi√° tr·ªã th·ª±c t·∫ø b∆∞·ªõc 6\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        filtered_times,\n",
    "        filtered_predictions[:, 5],\n",
    "        label=\"Gi√° tr·ªã d·ª± ƒëo√°n b∆∞·ªõc 6\",\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.xlabel(\"Th·ªùi gian\")\n",
    "    plt.ylabel(\"C√¥ng su·∫•t\")\n",
    "    plt.title(\"So s√°nh gi√° tr·ªã th·ª±c t·∫ø v√† d·ª± ƒëo√°n (B∆∞·ªõc 1 v√† B∆∞·ªõc 6, 5h-19h)\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    def calculate_mape(true, pred):\n",
    "        mask = true != 0\n",
    "        return (\n",
    "            np.mean(np.abs((true[mask] - pred[mask]) / true[mask])) * 100\n",
    "            if mask.any()\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "    mape_step1 = calculate_mape(filtered_true_values[:, 0], filtered_predictions[:, 0])\n",
    "    mape_step6 = calculate_mape(filtered_true_values[:, 5], filtered_predictions[:, 5])\n",
    "\n",
    "    print(f\"‚úÖ Sai s·ªë ph·∫ßn trƒÉm (MAPE) b∆∞·ªõc 1: {mape_step1:.2f}%\")\n",
    "    print(f\"‚úÖ Sai s·ªë ph·∫ßn trƒÉm (MAPE) b∆∞·ªõc 6: {mape_step6:.2f}%\")\n",
    "\n",
    "    return true_values, predictions\n",
    "\n",
    "\n",
    "test_start_date = datetime.datetime(2025, 1, 15)\n",
    "test_end_date = datetime.datetime(2025, 1, 15)\n",
    "true_values, predictions = test_model(model, scaler, test_start_date, test_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(true_values[:, 23], label=\"Gi√° tr·ªã th·ª±c t·∫ø b∆∞·ªõc 6\", color=\"red\")\n",
    "plt.plot(\n",
    "    predictions[:, 23], label=\"Gi√° tr·ªã d·ª± ƒëo√°n b∆∞·ªõc 6\", color=\"red\", linestyle=\"--\"\n",
    ")\n",
    "# plt.plot(filtered_times, filtered_true_values[:, 5], label='Gi√° tr·ªã th·ª±c t·∫ø b∆∞·ªõc 6', color='red')\n",
    "# plt.plot(filtered_times, filtered_predictions[:, 5], label='Gi√° tr·ªã d·ª± ƒëo√°n b∆∞·ªõc 6', color='red', linestyle='--')\n",
    "plt.xlabel(\"Th·ªùi gian\")\n",
    "plt.ylabel(\"C√¥ng su·∫•t\")\n",
    "plt.title(\"So s√°nh gi√° tr·ªã th·ª±c t·∫ø v√† d·ª± ƒëo√°n (B∆∞·ªõc 6)\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
